{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     47\u001b[0m imputation_feature_groups \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetasq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdolvol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpricedelay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_dolvol\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerotrade\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom1m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbm_ia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcashdebt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcashpr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcinvest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtb\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     59\u001b[0m }\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Apply imputations\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimputation_feature_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Handle missing values for 'chcsho' and 'convind'\u001b[39;00m\n\u001b[1;32m     65\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchcsho\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchcsho\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Use median for numerical feature\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mapply_imputation\u001b[0;34m(data, feature_groups)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     37\u001b[0m     knn_imputer \u001b[38;5;241m=\u001b[39m KNNImputer(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     data[valid_features] \u001b[38;5;241m=\u001b[39m \u001b[43mknn_imputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterative\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     40\u001b[0m     iterative_imputer \u001b[38;5;241m=\u001b[39m IterativeImputer(estimator\u001b[38;5;241m=\u001b[39mBayesianRidge(), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/impute/_knn.py:367\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    359\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    360\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    366\u001b[0m )\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2181\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2180\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2181\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/impute/_knn.py:350\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[0;34m(dist_chunk, start)\u001b[0m\n\u001b[1;32m    345\u001b[0m     dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[1;32m    346\u001b[0m         :, potential_donors_idx\n\u001b[1;32m    347\u001b[0m     ]\n\u001b[1;32m    349\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors, \u001b[38;5;28mlen\u001b[39m(potential_donors_idx))\n\u001b[0;32m--> 350\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_impute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m X[receivers_idx, col] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/impute/_knn.py:184\u001b[0m, in \u001b[0;36mKNNImputer._calc_impute\u001b[0;34m(self, dist_pot_donors, n_neighbors, fit_X_col, mask_fit_X_col)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to impute a single column.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Imputed values for receiver.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Get donors\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m donors_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_pot_donors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    185\u001b[0m     :, :n_neighbors\n\u001b[1;32m    186\u001b[0m ]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get weight matrix from distance matrix\u001b[39;00m\n\u001b[1;32m    189\u001b[0m donors_dist \u001b[38;5;241m=\u001b[39m dist_pot_donors[\n\u001b[1;32m    190\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(donors_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], donors_idx\n\u001b[1;32m    191\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:908\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    830\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m \n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"return_predictability_data_2009to2021.csv\"  # Adjust the path as needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert DATE to datetime\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "# Sort data by DATE\n",
    "data = data.sort_values(by='DATE').reset_index(drop=True)\n",
    "\n",
    "# Extract temporal components: year, month, quarter\n",
    "data['year'] = data['DATE'].dt.year\n",
    "data['month'] = data['DATE'].dt.month\n",
    "data['quarter'] = data['DATE'].dt.quarter\n",
    "\n",
    "\n",
    "# Imputation Function\n",
    "def apply_imputation(data, feature_groups):\n",
    "    \"\"\"\n",
    "    Apply imputation methods dynamically based on feature groups.\n",
    "    \"\"\"\n",
    "    for method, features in feature_groups.items():\n",
    "        valid_features = [feature for feature in features if feature in data.columns]\n",
    "        if method == 'mean':\n",
    "            data[valid_features] = data[valid_features].fillna(data[valid_features].mean())\n",
    "        elif method == 'linear':\n",
    "            data[valid_features] = data[valid_features].interpolate(method='linear', axis=0)\n",
    "        elif method == 'knn':\n",
    "            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "            data[valid_features] = knn_imputer.fit_transform(data[valid_features])\n",
    "        elif method == 'iterative':\n",
    "            iterative_imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=0)\n",
    "            data[valid_features] = iterative_imputer.fit_transform(data[valid_features])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Apply Imputation\n",
    "imputation_feature_groups = {\n",
    "    'mean': ['beta', 'betasq', 'dolvol', 'pricedelay', 'ill', 'std_dolvol',\n",
    "            'zerotrade', 'mom1m', 'bm', 'bm_ia', 'cashdebt', 'cashpr', 'cfp',\n",
    "             'cfp_ia', 'chinv', 'depr', 'divo', 'dy', 'ep', 'gma', 'herf', 'ps', 'quick',\n",
    "             'roic', 'salerec'],\n",
    "    'linear': ['chmom', 'mom6m', 'mom12m', 'mom36m', 'chcsho', 'convind'],\n",
    "    'knn': ['absacc', 'agr', 'chempia', 'currat', 'hire', 'invest', 'lgr',\n",
    "            'mve_ia', 'pchcapx_ia', 'pchcurrat', 'pchdepr', 'pchgm_pchsale', 'pchquick', \n",
    "            'saleinv', 'sgr', 'sp', 'tang', 'idiovol', 'turn', \n",
    "            'std_turn', 'baspread'],\n",
    "    'iterative': ['rd_mve', 'pctacc', 'orgcap', 'acc', 'chatoia', 'grcapx',\n",
    "                  'cinvest', 'rsup', 'tb']\n",
    "}\n",
    "\n",
    "# Apply imputations\n",
    "data = apply_imputation(data, imputation_feature_groups)\n",
    "\n",
    "# Handle missing values for 'chcsho' and 'convind'\n",
    "data['chcsho'].fillna(data['chcsho'].median(), inplace=True)  # Use median for numerical feature\n",
    "data['convind'].fillna(data['convind'].mode()[0], inplace=True)\n",
    "\n",
    "# Replace missing labels (e.g., 'missing' with 'Unknown') for categorical features\n",
    "data['sic2'] = data['sic2'].replace('missing', 'Unknown')\n",
    "\n",
    "# Print the number of features and rows in the final dataset\n",
    "print(f\"\\nFinal Dataset: {data.shape[0]} rows, {data.shape[1]} features.\")\n",
    "\n",
    "#Checking missing data\n",
    "print(\"Missing values after imputation:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "\n",
    "# Check for missing values after imputation\n",
    "missing_summary = data.isnull().sum()\n",
    "missing_count = missing_summary[missing_summary > 0]\n",
    "\n",
    "if not missing_count.empty:\n",
    "    print(\"\\nFeatures with Missing Values After Imputation:\")\n",
    "    print(missing_count)\n",
    "else:\n",
    "    print(\"\\nNo missing values found after imputation!\")\n",
    "\n",
    "\n",
    "# Save processed dataset\n",
    "output_path = \"processed_data_2009to2021_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Processed dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73125/2097780688.py:63: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['chcsho'].fillna(data['chcsho'].median(), inplace=True)  # Use median for numerical feature\n",
      "/tmp/ipykernel_73125/2097780688.py:64: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['convind'].fillna(data['convind'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping features with remaining missing data: ['mvel1', 'chmom', 'mom6m', 'mom12m', 'mom36m', 'acc', 'chatoia', 'chempia', 'chpmia', 'divi', 'egr', 'grcapx', 'grltnoa', 'lev', 'mve_ia', 'operprof', 'orgcap', 'pchsale_pchinvt', 'pchsale_pchrect', 'pchsale_pchxsga', 'pchsaleinv', 'pctacc', 'rd', 'rd_mve', 'rd_sale', 'realestate', 'salecash', 'secured', 'securedind', 'sin', 'tb', 'aeavol', 'chtx', 'cinvest', 'ear', 'nincr', 'roavol', 'rsup', 'stdacc', 'stdcf', 'ms', 'maxret', 'retvol']\n",
      "\n",
      "Final Dataset: 890593 rows, 67 features.\n",
      "Missing values after dropping features:\n",
      "permno        0\n",
      "DATE          0\n",
      "beta          0\n",
      "betasq        0\n",
      "dolvol        0\n",
      "             ..\n",
      "macro_dfy     0\n",
      "macro_svar    0\n",
      "year          0\n",
      "month         0\n",
      "quarter       0\n",
      "Length: 67, dtype: int64\n",
      "Imputation completed and dataset saved to processed_sample_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"return_predictability_data_2009to2021.csv\"  # Adjust the path as needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert DATE to datetime\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "# Sort data by DATE\n",
    "data = data.sort_values(by='DATE').reset_index(drop=True)\n",
    "\n",
    "# Extract temporal components: year, month, quarter\n",
    "data['year'] = data['DATE'].dt.year\n",
    "data['month'] = data['DATE'].dt.month\n",
    "data['quarter'] = data['DATE'].dt.quarter\n",
    "\n",
    "\n",
    "# Imputation Function\n",
    "def apply_imputation(data, feature_groups):\n",
    "    \"\"\"\n",
    "    Apply imputation methods dynamically based on feature groups.\n",
    "    \"\"\"\n",
    "    for method, features in feature_groups.items():\n",
    "        valid_features = [feature for feature in features if feature in data.columns]\n",
    "        if method == 'mean':\n",
    "            data[valid_features] = data[valid_features].fillna(data[valid_features].mean())\n",
    "        elif method == 'median':\n",
    "            data[valid_features] = data[valid_features].fillna(data[valid_features].median())\n",
    "        elif method == 'mode':\n",
    "            for feature in valid_features:\n",
    "                data[feature] = data[feature].fillna(data[feature].mode()[0])\n",
    "        elif method == 'knn':\n",
    "            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "            data[valid_features] = knn_imputer.fit_transform(data[valid_features])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Redistributed Features for Imputation (without KNN)\n",
    "imputation_feature_groups = {\n",
    "    'mean': ['beta', 'betasq', 'dolvol', 'pricedelay', 'ill', 'std_dolvol',\n",
    "             'zerotrade', 'mom1m', 'bm', 'bm_ia', 'cashdebt', 'cashpr', 'cfp',\n",
    "             'cfp_ia', 'chinv', 'depr', 'divo', 'dy', 'ep', 'gma', 'herf', 'ps', \n",
    "             'quick', 'roic', 'salerec', 'absacc', 'agr', 'currat', 'hire', \n",
    "             'invest', 'lgr', 'pchcapx_ia', 'pchcurrat', 'pchdepr', \n",
    "             'pchgm_pchsale', 'pchquick', 'saleinv', 'sgr', 'sp', 'tang', \n",
    "             'idiovol', 'turn', 'std_turn', 'baspread', 'cash', 'roaq', 'roeq', 'age'],\n",
    "    'linear': ['chmom', 'mom6m', 'mom12m', 'mom36m', 'chcsho', 'convind'],\n",
    "    'iterative': ['rd_mve', 'pctacc', 'orgcap', 'acc', 'chatoia', 'grcapx',\n",
    "                  'cinvest', 'rsup', 'tb']\n",
    "}                  \n",
    "\n",
    "# Apply imputations\n",
    "data = apply_imputation(data, imputation_feature_groups)\n",
    "\n",
    "# Handle missing values for 'chcsho' and 'convind'\n",
    "data['chcsho'].fillna(data['chcsho'].median(), inplace=True)  # Use median for numerical feature\n",
    "data['convind'].fillna(data['convind'].mode()[0], inplace=True)\n",
    "\n",
    "# Replace missing labels (e.g., 'missing' with 'Unknown') for categorical features\n",
    "data['sic2'] = data['sic2'].replace('missing', 'Unknown')\n",
    "\n",
    "# Identify and drop features with remaining missing data\n",
    "missing_summary = data.isnull().sum()\n",
    "features_to_drop = missing_summary[missing_summary > 0].index.tolist()\n",
    "if features_to_drop:\n",
    "    print(f\"\\nDropping features with remaining missing data: {features_to_drop}\")\n",
    "    data = data.drop(columns=features_to_drop)\n",
    "\n",
    "# Check for duplicate features and drop them\n",
    "# Check for duplicate features and drop them\n",
    "def drop_duplicate_features(data):\n",
    "    \"\"\"\n",
    "    Identify and drop duplicate features based on their values.\n",
    "    \"\"\"\n",
    "    duplicate_features = []\n",
    "    cols = data.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            if data[cols[i]].equals(data[cols[j]]):  # Check if two columns are identical\n",
    "                duplicate_features.append(cols[j])\n",
    "    duplicate_features = list(set(duplicate_features))  # Remove duplicates from the list\n",
    "    if duplicate_features:\n",
    "        print(f\"\\nDropping duplicate features: {duplicate_features}\")\n",
    "        data = data.drop(columns=duplicate_features)\n",
    "    return data\n",
    "\n",
    "# Print the number of features and rows in the final dataset\n",
    "print(f\"\\nFinal Dataset: {data.shape[0]} rows, {data.shape[1]} features.\")\n",
    "print(\"Missing values after dropping features:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Save the engineered dataset\n",
    "output_path = \"processed_sample_dataset.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Imputation completed and dataset saved to {output_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
